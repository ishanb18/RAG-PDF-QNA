# ğŸ“˜ RAG-PDF-QnA â€” Retrieval-Augmented Generation for Intelligent PDF Question Answering

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline that enables users to upload PDF documents and interactively ask context-aware questions.  
The system retrieves relevant sections from the document using **vector embeddings** and generates accurate, human-like answers using **LLMs (Large Language Models)**.

---

## ğŸš€ Features

- ğŸ“„ Upload any PDF and ask natural-language questions.  
- ğŸ§  Combines **retrieval-based search** with **generative AI** for precise answers.  
- ğŸ” Uses **FAISS** vector database for semantic search.  
- ğŸ§© Built with **LangChain**, **OpenAI embeddings**, and **FastAPI** for modular, real-time interaction.  
- ğŸ’¬ Supports both CLI and web API usage.  

---

## ğŸ—ï¸ Tech Stack & Architecture

| Layer | Technology | Purpose |
|-------|-------------|----------|
| **Frontend / Interface** | Streamlit or FastAPI UI | File upload, question input |
| **Backend Framework** | FastAPI | Manages routes, queries, and response handling |
| **Document Processing** | PyPDF2 / LangChain | Extracts and chunks text from PDFs |
| **Vector Storage** | FAISS | Stores document embeddings for semantic retrieval |
| **Embeddings Model** | OpenAI `text-embedding-ada-002` | Converts document chunks into dense vectors |
| **LLM for Generation** | GPT-3.5 / GPT-4 (via OpenAI API) | Generates context-based answers |
| **Pipeline Control** | LangChain | Orchestrates retrieval + generation workflow |

---

## ğŸ”„ Workflow Overview

```text
ğŸ“ PDF Upload
   â†“
ğŸ“„ Text Extraction â†’ Chunking â†’ Embedding Generation
   â†“
ğŸ§® Embeddings stored in FAISS Vector DB
   â†“
â“ User Question
   â†“
ğŸ” Retrieve top relevant chunks (semantic search)
   â†“
ğŸ’¬ Combine retrieved context + user query
   â†“
ğŸ¤– Generate final answer using OpenAI LLM
   â†“
ğŸ“¤ Return answer to user
